# 损失函数及最优化

## 系列简介

本系列将介绍Stanford深度学习课程的主要内容，做以下说明：本系列不是对原视频翻译，而是对每一课的主要内容进行说明及补充，其中关于课程中的行政事务我们将不做说明。 由于水平有限，在文章中难免出现错误，希望各位多多包含，帮助指正。

本文为该课程第三篇，介绍损失函数及最优化的相关内容。


## 线性分类模型

在前面图像分类流程中，我们介绍了线性分类模型，可以将线性分类模型简化为![equation](http://latex.codecogs.com/gif.latex?Y=Wx)。容易知道，对于训练数据来说![equation](http://latex.codecogs.com/gif.latex?Y)和![equation](http://latex.codecogs.com/gif.latex?x)都是已知的，但是![equation](http://latex.codecogs.com/gif.latex?W)是未知的。前面一节的内容中是直接给出了一组![equation](http://latex.codecogs.com/gif.latex?W)的值，那么问题来了：这组权重是怎么得出来的？这组权重好吗？如果不好，怎么得到更好的权重？

本节的主要内容主要包括：

1. 如何评估已有的一组权重的好坏？损失函数（loss function）
2. 如何得到并优化权重？最优化（Optimization）


## 损失函数

首先说一个例子，比如在CIFAR-10中，要对图片进行分类，选择十个分类中的一个。根据前面的线性分类模型，![equation](http://latex.codecogs.com/gif.latex?Y=Wx)应该是一个长度为10的向量，这10个值分别表示这个图片为对应着的分类的数值。比如下图为分别对猫、轿车、青蛙进行分类的一个结果，可以看到第一张图片被分类为猫的数值为2.9，但是这列中最大值为8.02，所以这张猫的图片被错误的分类成了狗。同样的，第二张照片被分类成轿车的数值是6.04，为这一列中最大值，所以该图片正确分类。第三张照片被分类成青蛙的数值为-4.34，而最大值为6.14，显然是错误分类。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture3/0.png)

所以上面的分类结果对应的权重矩阵对于中间的轿车是好的，但是对于猫和青蛙就是不好的。根据数值来说，我们直觉上可以觉得，猫的分类效果比青蛙好一些，因为我们可以看到猫的数值2.9距离最大值8.02差距只有大概5.12，而青蛙的数值-4.34距离最大值6.14有10.48，猫相对来说要准确一些。但是这个只是直觉，损失函数做的事情就是讲分类正确性的这种“直觉”量化出来。或者这么说，如果分类正确了，我们就很开心，如果分类不正确，我们就不开心，分类错的越严重，我们不开心程度就越重，此时损失函数就是用来衡量我们不开心的程度的函数。

为了更加清晰的说明损失函数，从上面的10分类问题变成研究3分类问题，只分类图片属于猫、汽车、青蛙中的哪一个。这里设置的数值和上面例子类似，猫和青蛙都没有被正确分类，但是直觉上猫的情况会比青蛙好一些，而汽车被正确分类。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture3/1.png)

损失函数定义如下：若是设定一个数据集![equation](http://latex.codecogs.com/gif.latex?\left\{(x_i,y_i)\right\}_{i=1}^{N}),其中![equation](http://latex.codecogs.com/gif.latex?X_i)为输入图片（数据），![equation](http://latex.codecogs.com/gif.latex?Y_i)为该图片（数据）所属分类（标签），![equation](http://latex.codecogs.com/gif.latex?N)为数据集中样本数量。在此基础上可以定义预测函数为

![equation](http://latex.codecogs.com/gif.latex?f(X_i,W)),

单个样本的损失定义为

![equation](http://latex.codecogs.com/gif.latex?L_i(f(X_i,W),Y_i))，

用于衡量预测值与真实值的差异，然后损失函数可以定义为

![equation](http://latex.codecogs.com/gif.latex?\frac{1}{N}\sum_{i}L_i(f(X_i,W),Y_i))，

衡量当前参数对于在所有的数据集中的平均损失程度，一个好的分类器，既然名字叫损失，当然希望损失越小越好，所以在定义![equation](http://latex.codecogs.com/gif.latex?L_i)的时候需要把握若是当预测正确时，没有损失，若是预测错得越厉害，损失越大。

## SVM损失函数

一种常用的损失函数是Hinge损失函数，SVM算法中使用的就是这个损失函数，所以也叫SVM损失函数。对每一个输入的图片，可以通过预测函数![equation](http://latex.codecogs.com/gif.latex?f(X_i,W))预测得到该图片属于每一个分类的数值，这里还是以上面说的三分类来说明，所以预测结果应该是长度为3的向量。下面令

![equation](http://latex.codecogs.com/gif.latex?s=f(X_i,W))

并且![equation](http://latex.codecogs.com/gif.latex?Y_i=1,2,3)中的其中一个，代表![equation](http://latex.codecogs.com/gif.latex?Y_i)属于第一分类、第二分类、第三分类。SVM损失函数定义为：

![equation](http://latex.codecogs.com/gif.latex?L_i=\sum_{j\ne{y_i}}(0,s_j-s_{y_i}+1))

其含义为，对于某个样本来说，在三个分类中，有一个正确类别，两个错误类别。对于每一个错误类别，若是正确类别的数值比该错误类别的数值至少大于1，那么认为这个错误分类的损失为0，否则认为其损失为该错误类别数值与正确类别数值之差再减1。也就是说若是错误类别数值相对于正确类别数值越大，损失也越大。三个类别中有两个是错误类别，所以两个错误类别的损失加起来也就是当前这个样本的损失了。当然这里的定义的要求正确数值比错误类别数值大1，这里的1是可以根据实际数据情况自己定义的。

说这么多，说得也不清楚，不如直接看个例子，还是刚才的三分类问题，如下图所示：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture3/1.png)

对于第一个样本猫来说，正确分类是第一个分类，第二个类别和第三个类别都是错误分类。所以第一个样本的损失为：

![equation](http://latex.codecogs.com/gif.latex?L_1=max(0,5.1-3.2+1)+max(0,-1.7-3.2+1))

![equation](http://latex.codecogs.com/gif.latex?=max(0,2.9)+max(0,-3.9)=2.9+0=2.9)

对于第二个样本猫来说，正确分类是第二个分类，第一个类别和第三个类别都是错误分类。所以第二个样本的损失为：

![equation](http://latex.codecogs.com/gif.latex?L_2=max(0,1.3-4.9+1)+max(0,2.0-4.9+1))

![equation](http://latex.codecogs.com/gif.latex?=max(0,-2.6)+max(0,-1.9)=0+0=0)

对于第三个样本猫来说，正确分类是第三个分类，第一个类别和第二个类别都是错误分类。所以第三个样本的损失为：

![equation](http://latex.codecogs.com/gif.latex?L_3=max(0,2.2-(-3.1)+1)+max(0,2.5-(-3.1)+1))

![equation](http://latex.codecogs.com/gif.latex?=max(0,6.3)+max(0,6.6)=6.3+6.6=12.9)

所以对于所有的三个样本来说，其损失函数为三个样本上损失的平均值：

![equation](http://latex.codecogs.com/gif.latex?L=(L_1+L_2+L_3)/3=(2.9+0+12.9)/3=5.27)

所以对于这个样例来说其损失函数值为5.27，通过这个例子应该就很清楚SVM损失函数是如何计算的了。

## SVM损失函数的一些思考

下面思考一些问题：

1.如果上述汽车那一列的预测值有微小的变化会产生什么结果？
> 损失函数值不会有变化，因为SVM损失衡量的是错误分类数值与正确分类数值的差异是否大于1，由于汽车是被正确分类的（正确分类数值别其他分类大较多），所以若只是做微小变化不会对分类的正确性产生变化。

2.上面损失函数的最小值，最大值分别是什么？
>显然最小值应该是0，只需要对所有的数据都正确分类，且正确分类的数值相比与错误分类的数值都大很多（这里大于1）。而最大值可以达到无穷大，只需要将错误分类数值设置得无穷大，正确类别的数值为0或者其他较小数就可以了。

3.若是所有样本对每个分类的数值都为0，或者说都基本相等，此时损失函数值为多少？
>此时损失函数为分类类别总数减一，这里是3-1=2。因为对于正确分类和错误分类的数值都一样，所以根据SVM损失函数的公式可以直接计算得到。 

4.注意到SVM损失函数在计算的时候是对所有的错误类别与正确类别进行比较计算，若是将所有的类别（包括正确类别）都进行比较呢？损失函数会有什么变化？
>损失函数会在原来的基础上加1，在实际分析中，我们采用前一种方法，是因为我们期望当都正确分类时，损失函数要为0。

5.SVM损失函数在对某一个样本进行计算损失的时候，我们要将所有错误类别的损失加和，若是这里是取平均会发生什么情况？
>本质上不会改变损失函数的性质，仅仅是在原来损失函数的基础上除了一个固定值（类别数量-1）

6.如何如下面公式这样定义样本的损失会产生什么改变？会产生一个新的分类器吗？

![equation](http://latex.codecogs.com/gif.latex?L_i=\sum_{j\ne{y_i}}(0,s_j-s_{y_i}+1)^2)

>这样会产生一个新的损失函数。

## SVM损失函数 python code

```python
import numpy as np

def L_i(X, Y, W):
    """
    X is the input images with size M * 1, M is the number of pixels
    Y is the labels of input X
    W is the weight matrix with size k * M, k is the number of classes the input should be categoried into.
    """
    scores = W.dot(X)  # scores would be k * 1size
    margins = np.maximum(0, scores-scores[Y]+1)
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i
    
```

大家可以自己试试代码哦。

## SVM损失函数最优解

SVM损失函数为：

![equation](http://latex.codecogs.com/gif.latex?{\frac{1}{N}{\sum_{i}^{N}}{\sum_{j\ne{y_i}}}max(0,f(X_i,W)_{j}-f(X_i,W)_{y_i}+1)})

最开始说了损失函数是用来衡量我们的不开心程度的，我们当然是越开心越好啦。所以我们希望上面的损失函数值最好为0，那就完美了。然后就有一个问题了，使得
上述损失函数值为0的W的解是唯一的吗？

>不是唯一解。假如W是一组可以使得损失函数为0的权重，由于所有的关系都是线性关系，我们如果将所有的W都扩大为原来的2倍，即2W也能使得损失函数值为0。所以应该是有无数组最优解的。

拿刚才的例子来说一下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture3/1.png)

第二列是对汽车的预测值。我们可以计算损失函数为：

![equation](http://latex.codecogs.com/gif.latex?L_2=max(0,1.3-4.9+1)+max(0,2.0-4.9+1))

![equation](http://latex.codecogs.com/gif.latex?=max(0,-2.6)+max(0,-1.9)=0+0=0)

若是所有的W都变为2W，所以第二列所有的值都会扩大为原来的2倍，损失为：

![equation](http://latex.codecogs.com/gif.latex?L_2=max(0,2.6-9.8+1)+max(0,4.0-9.8+1))

![equation](http://latex.codecogs.com/gif.latex?=max(0,-6.2)+max(0,-4.8)=0+0=0)

可以看到2W和W都能使得这个样本的损失为0。

## 正则化

上面说到存在无数个W可以使得损失函数达到最小值0，那么我们在使用的时候应该使用哪个呢？通过上面的例子我们可以知道，损失函数是对训练数据来说的，在实际应用中，我们最希望的可能不是我们的分类器在训练集上表现得有多好，而是想看其在验证集及测试集上表现得有多好。所以通常的情况是，我们不需要将上述的损失函数达到0。下面举个例子来说明：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture3/2.png)

在上图中，蓝色的圆点为训练数据，绿色的方点为测试数据。若是我们训练一个得到一个分类器在训练集上的损失为0，如上图中蓝色曲线拟合的函数。此时对于蓝色的训练数据来说，该曲线非常完美的表达了训练集的内容。但是对于绿色的测试集来说，预测的效果就非常不好。这个时候就出现了我们通常所说的“过拟合”,我们可能更倾向于使用绿色的直线用来做拟合。

在损失函数后面加一个正则项就是我们通常说的正则化，公式如下：

![equation](http://latex.codecogs.com/gif.latex?{\frac{1}{N}{\sum_{i}^{N}}L_i(f(X_i,W),Y_i)+\lambda{R(W)}})

上面这种损失函数是机器学习中非常常见的损失函数，前面训练数据的损失值加上后面的正则项，正则项的主要目的是想要模型变得简单。正则项中的lambda是一个超参数。

## 常用的正则化

常用的正则化主要包括L2正则化，L1正则化以及Elastic Net正则化（L1+L2），他们的公式如下：

1.L2正则化

![equation](http://latex.codecogs.com/gif.latex?R(W)=\sum_k\sum_l{W_{k,l}^2})

L2正则化后面加上的是所有参数W的平放和，若是较大的lambda值会使得所有的W都较小，从而增加模型的稳定性。

2.L1正则化

![equation](http://latex.codecogs.com/gif.latex?R(W)=\sum_k\sum_l\abs{W_{k,l}})

L1正则化也较为常用，线性回归中的LASSO回归就是在基本线性回归的基础上加上L1正则化。

3.Elastic Net正则化（L1+L2）

![equation](http://latex.codecogs.com/gif.latex?R(W)=\sum_k\sum_l(\beta{W_{k,l}^2}+\abs{W_{k,l}}))

同时结合L1正则化和L2正则化的优势。

## L1，L2正则化

L1正则化的效果是尽量将参数变得稀疏，也就是将很多参数都变为0，而L2正则化则是尽量将各个参数都变得平均。举个例子（课程中的例子只是说明了L2的需要更加平均，并不能很好的介绍L1的效果，这里我重新举个例子）：

假如:

![equation](http://latex.codecogs.com/gif.latex?X=(1,1,2,1))

![equation](http://latex.codecogs.com/gif.latex?W_1=(0,0,1,0))

![equation](http://latex.codecogs.com/gif.latex?W_2=(0.4,0.4,0.4,0.4))

显然有

![equation](http://latex.codecogs.com/gif.latex?W_{1}^{T}X=W_{2}^{T}=2)

也就是说，在不加正则项时，两个W的效果一直。我们可以分别计算L1和L2正则项，

![equation](http://latex.codecogs.com/gif.latex?R_{L_1}(W_1)=0+0+1+0=1)

![equation](http://latex.codecogs.com/gif.latex?R_{L_1}{W_2}=0.4+0.4+0.4+0.4=1.6)

显然在L1正则化情况下，W1会是更好的参数。

![equation](http://latex.codecogs.com/gif.latex?R_{L_1}(W_1)=0^2+0^2+1^2+0^2=1)

![equation](http://latex.codecogs.com/gif.latex?R_{L_1}{W_2}=0.4^2+0.4^2+0.4^2+0.4^2=0.64)

在L2正则化的条件下，W2会是更好的参数。

## Softmax分类器

Softmax损失函数在Deep learning中更加常用，下面将介绍Softmax损失函数。

在前面的SVM损失函数中，每一个样本可以得到一个3个数值（分别代表这个样本属于三个分类的打分），但是这些打分并没有什么意义，可以是正的，也可能是负的，我们只知道需要使得正确类别的数值高于错误类别的数值就是好的。所以我们需要介绍Softmax分类器，说softmax大家可能不知道是什么，但是给他换个名字可能就理解了，也叫多分类的logistic回归。在Softmax中，返回的数值不再是无意义的数值，而是通过转化，将每一个类别的数值转化为样本为该类别的概率。转化公式(Softmax函数)如下：

![equation](http://latex.codecogs.com/gif.latex?P(Y=k|X=X_i)=\frac{e^{s_k}}{\sum_j{e^{s_j}}})

其中，

![equation](http://latex.codecogs.com/gif.latex?s=f(Xi,W))

对于每一个样本来说，我们肯定是想将上述的概率最大化，也就是想要最大化其对数似然率，又或者说是最小化其负对数似然率，这个就是Softmax对于单个样本的损失，计算方式如下：


![equation](http://latex.codecogs.com/gif.latex?L_i=-\log{P(Y=Y_i|X=X_i)}=-\log{\frac{e^{s_{y_i}}}{\sum_j{e^{s_j}}}})

下面看一下例子来理解Softmax损失：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture3/3.png)

还是使用前面的例子，但是我们主要关心第一列，猫的分类中的情况。我们看到，图片被分类为猫、汽车、青蛙的数值分别为3.2,5.1，-1.7。我们首先对其取指数值，分别得到24.5,164 0.18，然后对其归一化，分别得到属于猫、汽车、青蛙的概率为0.13,0.87,0.00，被分类为汽车的概率最大，为错误分类。而损失函数为正确分类概率的负对数，得到

![equation](http://latex.codecogs.com/gif.latex?L_i=-\log{(0.13)}=0.89)

## 关于Softmax损失函数的几个思考

1.Softmax损失函数的最小值最大值分别是多少？
>显然当正确类别的概率为1时，损失为0，所以最小值为0，什么时候能够取得正确类别的概率为1呢？显然应该是当正确类别的打分为无穷大，而错误类别的打分为无穷小的时候。当正确类别概率极小时，损失为无穷大，所以最大值为无穷大，此时打分情况刚好和最小值时相反。

2.若是所有的打分值都接近0，此时损失函数为多少？
>![equation](http://latex.codecogs.com/gif.latex?-\log{C})，C是类别数量


## 索引

上一讲： ![Stanford深度学习课程|第二课图像分类流程](https://github.com/NGSHotpot/deep-learning/blob/master/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%B5%81%E7%A8%8B.md)

下一讲： ![Stanford深度学习课程|第四课]()
