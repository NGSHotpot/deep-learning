# 神经网络及反向传播算法

## 系列简介

本系列将介绍Stanford深度学习课程的主要内容，做以下说明：本系列不是对原视频翻译，而是对每一课的主要内容进行说明及补充，其中关于课程中的行政事务我们将不做说明。 由于水平有限，在文章中难免出现错误，希望各位多多包含，帮助指正。

本文为该课程第四篇，介绍神经网络及反向传播算法（Backpropagation）。


## 预备知识

我们不直接介绍课程内容，首先介绍一些预备知识，这样可以更好的理解课程内容。下面我们介绍导数的基本概念及一些常用函数的导数。有些人大概在高中就学过导数，但是有些人没有学过，但是不管怎么样，大家在中学阶段的时候一定学过直线方程！那么不知道大家还记得不记得斜率，下面我们看一个例子，有一个函数方程为：![equation](http://latex.codecogs.com/gif.latex?y=2x+4)，其函数图像如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/001.png)

这里的斜率![equation](http://latex.codecogs.com/gif.latex?k=2)，这里的斜率就是上面直线在直线上每一点的导数。导数其实也可以理解为在某一点上函数值的变化情况（包括变化快慢及变化方向）。

对于任意一个函数![equation](http://latex.codecogs.com/gif.latex?f(x))，其在![equation](http://latex.codecogs.com/gif.latex?x_0)处的导数可以定义为：

![equation](http://latex.codecogs.com/gif.latex?f^{'}(x)=\frac{df(x)}{dx}=\lim_{h->0}\frac{f(x_0+h)-f(x_0)}{h})

下面不加证明的列出一些常用函数的导数形式及相关公式：

![equation](http://latex.codecogs.com/gif.latex?(ax^n)^{'}=\frac{d(ax^n)}{dx}=anx^{n-1})

![equation](http://latex.codecogs.com/gif.latex?(ln(x))^{'}=\frac{1}{x})

![equation](http://latex.codecogs.com/gif.latex?(e^x)^{'}=e^x)

![equation](http://latex.codecogs.com/gif.latex?(f(x)+g(x))^{'}=f^{'}(x)+g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)-g(x))^{'}=f^{'}(x)-g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)g(x))^{'}=f^{'}(x)g(x)+f(x)g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(\frac{f(x)}{g(x)})^{'}=\frac{f^{'}(x)g(x)-f(x)g^{'}(x)}{(g(x))^2})

然后还有比较重要的链式法则，若有一个函数：

![equation](http://latex.codecogs.com/gif.latex?y=f(u))

![equation](http://latex.codecogs.com/gif.latex?u=g(x))

那么可以得到：

![equation](http://latex.codecogs.com/gif.latex?\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx})



在简单介绍这部分知识后，我们来到反向传播算法。

## 反向传播算法

对于任意一个运算，我们可以通过构建计算图，然后利用链式法则前向求导，从而求出每一个变量的梯度。举个例子,假如有函数：

![function](http://latex.codecogs.com/gif.latex?f(x,y,z)=(x+y)z)

且

![equation](http://latex.codecogs.com/gif.latex?x=-2,y=5,z=-4)

根据运算法则，我们首先要计算![equation](http://latex.codecogs.com/gif.latex?x+y)，再计算乘法，所以可以令：

![equation](http://latex.codecogs.com/gif.latex?q=x+y)

![equation](http://latex.codecogs.com/gif.latex?f(q,z)=qz)

根据上述运算顺序画出如下计算图

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/002.png)

首先前向计算得到：

![equation](http://latex.codecogs.com/gif.latex?q=x+y=-2+5=3)

![equation](http://latex.codecogs.com/gif.latex?f=qz=3\times{-4}=-12)

下面我们看看反向梯度是如何计算的，我们需要计算的是：

![equation](http://latex.codecogs.com/gif.latex?\nabla{f}=[\frac{df}{dx},\frac{df}{dy},\frac{df}{dz}])

根据函数式及求导公式，我们首先可以得出以下结论：

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dz}=(qz)^{'}=q=3)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dq}=(qz)^{'}=z=-4)

![equation](http://latex.codecogs.com/gif.latex?\frac{dq}{dx}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{dq}{dy}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dx}=\frac{df}{dq}\frac{dq}{dx}=-4\times1=-4)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dy}=\frac{df}{dq}\frac{dq}{dy}=-4\times1=4)

所以最终求得梯度为：

![equation](http://latex.codecogs.com/gif.latex?\nabla{f}=[\frac{df}{dx},\frac{df}{dy},\frac{df}{dz}]=[-4,-4,3])

有了梯度之后就可以使用梯度下降法对参数进行更新了。

## 稍微复杂的一个例子

上面介绍了及其简单的一个例子，下面是稍微复杂的例子，也是在实际中会经常用到的例子：逻辑回归的梯度求解。需要求解梯度的函数如下：

![equation](http://latex.codecogs.com/gif.latex?f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}})

然后我们将上述函数画成计算图（原课程中的图截图过来后不是很清晰，故重画了），通过换元将上面复杂的式子写成简单的式子（特别方便求导的式子）：

![equation](http://latex.codecogs.com/gif.latex?q_0=w_0x_0)

![equation](http://latex.codecogs.com/gif.latex?q_1=w_1x_1)

![equation](http://latex.codecogs.com/gif.latex?s_1=q_0+q_1)

![equation](http://latex.codecogs.com/gif.latex?s_2=s_1+w_2)

![equation](http://latex.codecogs.com/gif.latex?a_1=-s_2)

![equation](http://latex.codecogs.com/gif.latex?a_2=e^{a_1})

![equation](http://latex.codecogs.com/gif.latex?a_3=1+a_2)

![equation](http://latex.codecogs.com/gif.latex?a_4=\frac{1}{a_3})

根据上面过程画出来图形如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/003.png)

给定一组已知的输入，比如：

![equation](http://latex.codecogs.com/gif.latex?w_0=2,x_0=-1,w_1=-3,x_1=-2,w_2=-3)

我们可以前向计算上面计算图中的每一个位置的输出值如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/004.png)

下面根据链式法则来求![equation](http://latex.codecogs.com/gif.latex?w_0,x_0,w_1,x_1,w_2)的梯度，实际应用中只会对系数求梯度，输入变量是固定的。

![equation](http://latex.codecogs.com/gif.latex?\frac{da_4}{da_3}=-\frac{1}{a_3^2},\frac{da_3}{da_2}=1,\frac{da_2}{da_1}=e^{a_1},\frac{da_1}{ds_2}=-1)

通过上面的式子我们可以得到：

![equation](http://latex.codecogs.com/gif.latex?\frac{da_4}{ds_2}=\frac{da_4}{da_3}\frac{da_3}{da_2}\frac{da_2}{da_1}\frac{da_1}{ds_2}=\frac{e^{a_1}}{a_3^2}=\frac{e^{-s_2}}{(1+e^{-s_2})^2}=\frac{1+e^{-s_2}-1}{1+e^{-s_2}}\frac{1}{1+e^{-s_2}}=\frac{1}{1+e^{-s_2}}(1-\frac{1}{1+e^{-s_2}})=a_4(1-a_4))

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{s_2}}{\partial{w_0}}=x_0,\frac{\partial{s_2}}{\partial{x_0}}=w_0,\frac{\partial{s_2}}{\partial{w_1}}=x_1,\frac{\partial{s_2}}{\partial{x_1}}=w_1,\frac{\partial{s_2}}{\partial{w_2}}=1)


则我们可以得到如下列的反向计算结果：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/005.png)

上图中的结果就是给前面的公式中带入具体的值计算出来的。一旦计算得到各个变量的梯度值之后，就可以执行梯度下降，从而对损失函数进行优化。

## 阶段小结

由上面的推导，对于上面示例中的函数，其反向传播梯度可以通过如下公式进行求解。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/006.png)


## 门

标题特意用得较为简单，在上面的计算图计算前向和反向传播时，计算图很小，分成了很多部分。在实际中，我们不可能每次都将问题分成这么多部分，在不同的前向反向函数中，很多函数是一样的。这一些类似的计算图可以组成门。比如说如果一个多层的神经网络每一层都使用sigmoid激活函数，那么在计算反向梯度时，就会多次按照如下图中红框中部分进行计算。所以可以综合的称呼这些为“sigmoid门” （sigmoid gate）。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/007.png)

上面图片中的公式也就是sigmoid gate反向传播的的梯度公式，下面介绍一些常用门。

下面是课程ppt中的一个例子，如下图所示，分别描述了add gate（加门，红色虚线框），max gate（最大门，绿色虚线框）和multiple gate（乘门，蓝色虚线框）。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/008.png)

图中有一些数字，绿色数字代表前向传播数值，红色数字代表反向传播梯度。

Add gate的作用是将梯度分发给两个其他分支，在上图中，梯度2.00分发给了两个分支，所以两个分支的梯度都是2.00。简单说明一下add gate的分发原理，假如有函数：

![equation](http://latex.codecogs.com/gif.latex?y=x_1+x_2)

并且我们已知某个目标函数对y的梯度，那么y的梯度可以直接分发给x1和x2，

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{y}}{\partial{x_1}}=1,\frac{\partial{y}}{\partial{x_2}}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J}}{\partial{x_1}}=\frac{\partial{J}}{\partial{y}}\frac{\partial{y}}{\partial{x_1}}=\frac{\partial{J}}{\partial{y}},\frac{\partial{J}}{\partial{x_2}}=\frac{\partial{J}}{\partial{y}}\frac{\partial{y}}{\partial{x_2}}=\frac{\partial{J}}{\partial{y}})

Max gate的作用是取两个分支中较大的数值，在上图中，下面绿色虚线框代表一个max gate的过程。在梯度反向传播过程中，对于两个分支中较大的值，该分支梯度等于当前梯度，而对于两个分支中较小的值，该分支的梯度等于0。

Multiple gate的作用是将两个数值相乘，如上图中蓝色虚线框所示。其分支的梯度为当前梯度乘上另外一个分支的数值，比如说：

![equation](http://latex.codecogs.com/gif.latex?y=x_1x_2)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{y}}{\partial{x_1}}=x_2,\frac{\partial{y}}{\partial{x_2}}=x_1)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{J}}{\partial{x_1}}=\frac{\partial{J}}{\partial{y}}\frac{\partial{y}}{\partial{x_1}}=\frac{\partial{J}}{\partial{y}}x_2,\frac{\partial{J}}{\partial{x_2}}=\frac{\partial{J}}{\partial{y}}\frac{\partial{y}}{\partial{x_2}}=\frac{\partial{J}}{\partial{y}}x_1)

上面为常用的一些“门”，add gate可以理解为一个分发门，max gate可以理解为一个路由门，multiple gate可以理解为一个转换门。

## 反向传播向量化表示

前面说的都是单个数据点的反向梯度求解，下面说对于向量或者矩阵来说梯度求解情况：首先假设需要计算的函数为：

![equation](http://latex.codecogs.com/gif.latex?f(x,W)=||W\cdot{x}||^2=\sum_{i}^{n}(W\cdot{x})_i^2)

为简便起见，我们设定W为大小为2x2的矩阵，而x为大小为2x1的向量：

![equation](http://latex.codecogs.com/gif.latex?W=\begin{bmatrix}W_{11}&W_{12}\\\W_{21}&W_{22}\end{bmatrix},x=\begin{bmatrix}x_1\\\x_2\end{bmatrix})

然后根据函数计算过程设定中间变量，并且画出计算图。

![equation](http://latex.codecogs.com/gif.latex?q=Wx=\begin{bmatrix}W_{11}x_{1}+W_{12}x_{2}\\\W_{21}x_1+W_{22}x_2\end{bmatrix}=\begin{bmatrix}q_1\\\q_2\end{bmatrix})

![equation](http://latex.codecogs.com/gif.latex?f(q)=||q||^2=q_{1}^{2}+q_{2}^2)

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/009.png)

然后我们通过实例来理解其前向运算及反向梯度传播，例如设定W和x的值分别为：

![equation](http://latex.codecogs.com/gif.latex?W=\begin{bmatrix}0.1&0.5\\\-0.3&0.8\end{bmatrix},x=\begin{bmatrix}0.2\\\0.4\end{bmatrix})

所以根据矩阵乘法计算可得到q的值为：

![equation](http://latex.codecogs.com/gif.latex?q=\begin{bmatrix}0.22\\\0.26\end{bmatirx})

然后计算f的函数值为：

![equation](http://latex.codecogs.com/gif.latex?f(q)=q_1^2+q_2^2=0.22^2+0.26^2=0.116)

下面是反向梯度传播：

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{f}}{\partial{f}}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{f}}{\partial{q_1}}=2q_1,\frac{\partial{f}}{\partial{q_2}}=2q_2)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{q_1}}{\partial{W_{11}}}=x_1,\frac{\partial{q_1}}{\partial{W_{12}}}=x_2,\frac{\partial{q_1}}{\partial{W_{21}}}=0,\frac{\partial{q_1}}{\partial{W_{22}}}=0)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{q_2}}{\partial{W_{11}}}=0,\frac{\partial{q_2}}{\partial{W_{12}}}=0,\frac{\partial{q_2}}{\partial{W_{21}}}=x_1,\frac{\partial{q_2}}{\partial{W_{22}}}=x_2)

![equation](http://latex.codecogs.com/gif.latex?\frac{\partial{q_1}}{\partial{x_1}}=W_{11},\frac{\partial{q_1}}{\partial{x_2}}=W_{12},\frac{\partial{q_2}}{\partial{x_1}}=W_{21},\frac{\partial{q_2}}{\partial{x_2}}=W_{22})

![equation](http://latex.codecogs.com/gif.latex?i_1=\begin{bmatrix}1\\\0\end{bmatrix},i_2=\begin{bmatrix}0\\\1\end{bmatrix})

所以对于梯度求导来说，有

![equation](http://latex.codecogs.com/gif.latex?\nabla_q{f}=2q=\begin{bmatrix}2q_1\\\2q_2\end{bmatrix})

![equation](http://latex.codecogs.com/gif.latex?\nabla_{W}q_1=i_1x^T=\begin{bmatrix}x_1&x_2\\\0&0\end{bmatrix})

![equation](http://latex.codecogs.com/gif.latex?\nabla_{W}q_2=i_2x^T=\begin{bmatrix}0&0\\\x_1&x_2\end{bmatrix})

![equation](http://latex.codecogs.com/gif.latex?\nabla_{W}f=\nabla_{q}f\nabla_{W}q=2qx^T=\begin{bmatrix}2q_1x_1&2q_1x_2\\\2q_2x_1&2q_2x_2\end{bmatrix})

![equation](http://latex.codecogs.com/gif.latex?\nabla_{x}f=2W^Tq)

下面继续说刚才的实例：下图为刚才说的例子中的前向传播与反向梯度传播的过程，我们一起来计算一下反向梯度传播的过程（红色数字部分）

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/010.png)

首先是函数f对自身的梯度为1，这个比较简单。然后是函数f对q的梯度：

![equation](http://latex.codecogs.com/gif.latex?\nabla_{q}f=\begin{bmatrix}2q_1\\\2q_2\end{bmatrix}=\begin{bmatrix}0.44\\\0.52\end{bmatrix})

然后是f对W的梯度：

![equation](http://latex.codecogs.com/gif.latex?\nabla_{W}f=2qx^T=\begin{bmatrix}2q_1\\\2q_2\end{bmatrix}\begin{bmatrix}x_1&x_2\end{bmatrix}=\begin{bmatrix}0.44\\\0.52\end{bmatrix}\begin{bmatrix}0.2&0.4\end{bmatrix}=\begin{bmatrix}0.088&0.176\\\0.104&0.208\end{bmatrix})

![equation](http://latex.codecogs.com/gif.latex?\nabla_{x}f=2W^Tq=2\begin{bmatrix}0.1&-0.3\\\0.5&0.8\end{bmatrix}\begin{bmatrix}0.22\\\0.26\end{bmatrix}=\begin{bmatrix}-0.112\\\0.636\end{bmatrix})

这样就可以较为顺畅的进行反向传播算法了。

## 神经网络

我们在上面的例子中使用的是

![equation](http://latex.codecogs.com/gif.latex?f=Wx)

这是一个线性函数，可以看成一层的神经网络，若是这里设定以ReLU做激活函数。我们可以通过叠加多个单单层网络构建两层甚至多层网络，比如：

![equation](http://latex.codecogs.com/gif.latex?f=W_2max(0,W_1x),f=W_3max(0,W_2max(0,W_1x)))

### 神经网络中的激活函数

神经网络中在神经元上会加上激活函数，激活函数可以引入非线性特征。常用的激活函数有Sigmoid，ReLU激活函数等。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/011.png)

上图中是很多的常用激活函数，包括函数的方程及其图像。比较早期的就是Sigmoid和tanh激活函数，从上面图像上我们可以看到，在输入值较大或者较小时，梯度很小，在多层网络中容易导致梯度消失，从而无法对多层网络进行很好的训练。所以就有了ReLU激活函数，ReLU激活函数在大于0的区域导数都为1，保证了梯度不会消失，且梯度计算速度极快。但是ReLU激活函数舍弃了小于的部分，会有信息丢失，然后就有了Leaky ReLU对于小于0的部分取梯度都为0.1，即保证了不丢弃小于0部分的信息，又保证了大于0部分的优势信息。但是Leaky ReLU在0点处导数不存在，所以在此基础上又提出了ELU激活函数。

## 索引

上一讲：[Stanford深度学习课程|第三课损失函数及最优化](https://github.com/NGSHotpot/deep-learning/blob/master/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8F%8A%E6%9C%80%E4%BC%98%E5%8C%96.md)

下一讲：[Stanford深度学习课程|第五课卷积神经网络](https://github.com/NGSHotpot/deep-learning/blob/master/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.md)
