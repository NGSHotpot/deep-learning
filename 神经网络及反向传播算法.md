# 神经网络及反向传播算法

## 系列简介

本系列将介绍Stanford深度学习课程的主要内容，做以下说明：本系列不是对原视频翻译，而是对每一课的主要内容进行说明及补充，其中关于课程中的行政事务我们将不做说明。 由于水平有限，在文章中难免出现错误，希望各位多多包含，帮助指正。

本文为该课程第四篇，介绍神经网络及反向传播算法（Backpropagation）。


## 预备知识

我们不直接介绍课程内容，首先介绍一些预备知识，这样可以更好的理解课程内容。下面我们介绍导数的基本概念及一些常用函数的导数。有些人大概在高中就学过导数，但是有些人没有学过，但是不管怎么样，大家在中学阶段的时候一定学过直线方程！那么不知道大家还记得不记得斜率，下面我们看一个例子，有一个函数方程为：![equation](http://latex.codecogs.com/gif.latex?y=2x+4)，其函数图像如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/001.png)

这里的斜率![equation](http://latex.codecogs.com/gif.latex?k=2)，这里的斜率就是上面直线在直线上每一点的导数。导数其实也可以理解为在某一点上函数值的变化情况（包括变化快慢及变化方向）。

对于任意一个函数![equation](http://latex.codecogs.com/gif.latex?f(x))，其在![equation](http://latex.codecogs.com/gif.latex?x_0)处的导数可以定义为：

![equation](http://latex.codecogs.com/gif.latex?f^{'}(x)=\frac{df(x)}{dx}=\lim_{h->0}\frac{f(x_0+h)-f(x_0)}{h})

下面不加证明的列出一些常用函数的导数形式及相关公式：

![equation](http://latex.codecogs.com/gif.latex?(ax^n)^{'}=\frac{d(ax^n)}{dx}=anx^{n-1})

![equation](http://latex.codecogs.com/gif.latex?(ln(x))^{'}=\frac{1}{x})

![equation](http://latex.codecogs.com/gif.latex?(e^x)^{'}=e^x)

![equation](http://latex.codecogs.com/gif.latex?(f(x)+g(x))^{'}=f^{'}(x)+g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)-g(x))^{'}=f^{'}(x)-g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)g(x))^{'}=f^{'}(x)g(x)+f(x)g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(\frac{f(x)}{g(x)})^{'}=\frac{f^{'}(x)g(x)-f(x)g^{'}(x)}{(g(x))^2})

在简单介绍这部分知识后，我们来到反向传播算法。

## 反向传播算法

对于任意一个运算，我们可以通过
