# 神经网络及反向传播算法

## 系列简介

本系列将介绍Stanford深度学习课程的主要内容，做以下说明：本系列不是对原视频翻译，而是对每一课的主要内容进行说明及补充，其中关于课程中的行政事务我们将不做说明。 由于水平有限，在文章中难免出现错误，希望各位多多包含，帮助指正。

本文为该课程第四篇，介绍神经网络及反向传播算法（Backpropagation）。


## 预备知识

我们不直接介绍课程内容，首先介绍一些预备知识，这样可以更好的理解课程内容。下面我们介绍导数的基本概念及一些常用函数的导数。有些人大概在高中就学过导数，但是有些人没有学过，但是不管怎么样，大家在中学阶段的时候一定学过直线方程！那么不知道大家还记得不记得斜率，下面我们看一个例子，有一个函数方程为：![equation](http://latex.codecogs.com/gif.latex?y=2x+4)，其函数图像如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/001.png)

这里的斜率![equation](http://latex.codecogs.com/gif.latex?k=2)，这里的斜率就是上面直线在直线上每一点的导数。导数其实也可以理解为在某一点上函数值的变化情况（包括变化快慢及变化方向）。

对于任意一个函数![equation](http://latex.codecogs.com/gif.latex?f(x))，其在![equation](http://latex.codecogs.com/gif.latex?x_0)处的导数可以定义为：

![equation](http://latex.codecogs.com/gif.latex?f^{'}(x)=\frac{df(x)}{dx}=\lim_{h->0}\frac{f(x_0+h)-f(x_0)}{h})

下面不加证明的列出一些常用函数的导数形式及相关公式：

![equation](http://latex.codecogs.com/gif.latex?(ax^n)^{'}=\frac{d(ax^n)}{dx}=anx^{n-1})

![equation](http://latex.codecogs.com/gif.latex?(ln(x))^{'}=\frac{1}{x})

![equation](http://latex.codecogs.com/gif.latex?(e^x)^{'}=e^x)

![equation](http://latex.codecogs.com/gif.latex?(f(x)+g(x))^{'}=f^{'}(x)+g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)-g(x))^{'}=f^{'}(x)-g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)g(x))^{'}=f^{'}(x)g(x)+f(x)g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(\frac{f(x)}{g(x)})^{'}=\frac{f^{'}(x)g(x)-f(x)g^{'}(x)}{(g(x))^2})

然后还有比较重要的链式法则，若有一个函数：

![equation](http://latex.codecogs.com/gif.latex?y=f(u))

![equation](http://latex.codecogs.com/gif.latex?u=g(x))

那么可以得到：

![equation](http://latex.codecogs.com/gif.latex?\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx})



在简单介绍这部分知识后，我们来到反向传播算法。

## 反向传播算法

对于任意一个运算，我们可以通过构建计算图，然后利用链式法则前向求导，从而求出每一个变量的梯度。举个例子,假如有函数：

![function](http://latex.codecogs.com/gif.latex?f(x,y,z)=(x+y)z)

且

![equation](http://latex.codecogs.com/gif.latex?x=-2,y=5,z=-4)

根据运算法则，我们首先要计算![equation](http://latex.codecogs.com/gif.latex?x+y)，再计算乘法，所以可以令：

![equation](http://latex.codecogs.com/gif.latex?q=x+y)

![equation](http://latex.codecogs.com/gif.latex?f(q,z)=qz)

根据上述运算顺序画出如下计算图

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/002.png)

首先前向计算得到：

![equation](http://latex.codecogs.com/gif.latex?q=x+y=-2+5=3)

![equation](http://latex.codecogs.com/gif.latex?f=qz=3\times{-4}=-12)

下面我们看看反向梯度是如何计算的，我们需要计算的是：

![equation](http://latex.codecogs.com/gif.latex?\nabla{f}=[\frac{df}{dx},\frac{df}{dy},\frac{df}{dz}])

根据函数式及求导公式，我们首先可以得出以下结论：

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dz}=(qz)^{'}=q=3)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dq}=(qz)^{'}=z=-4)

![equation](http://latex.codecogs.com/gif.latex?\frac{dq}{dx}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{dq}{dy}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dx}=\frac{df}{dq}\frac{dq}{dx}=-4\times1=-4)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dy}=\frac{df}{dq}\frac{dq}{dy}=-4\times1=4)

所以最终求得梯度为：

![equation](http://latex.codecogs.com/gif.latex?\nabla{f}=[\frac{df}{dx},\frac{df}{dy},\frac{df}{dz}]=[-4,-4,3])

有了梯度之后就可以使用梯度下降法对参数进行更新了。
