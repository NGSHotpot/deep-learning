# 神经网络及反向传播算法

## 系列简介

本系列将介绍Stanford深度学习课程的主要内容，做以下说明：本系列不是对原视频翻译，而是对每一课的主要内容进行说明及补充，其中关于课程中的行政事务我们将不做说明。 由于水平有限，在文章中难免出现错误，希望各位多多包含，帮助指正。

本文为该课程第四篇，介绍神经网络及反向传播算法（Backpropagation）。


## 预备知识

我们不直接介绍课程内容，首先介绍一些预备知识，这样可以更好的理解课程内容。下面我们介绍导数的基本概念及一些常用函数的导数。有些人大概在高中就学过导数，但是有些人没有学过，但是不管怎么样，大家在中学阶段的时候一定学过直线方程！那么不知道大家还记得不记得斜率，下面我们看一个例子，有一个函数方程为：![equation](http://latex.codecogs.com/gif.latex?y=2x+4)，其函数图像如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/001.png)

这里的斜率![equation](http://latex.codecogs.com/gif.latex?k=2)，这里的斜率就是上面直线在直线上每一点的导数。导数其实也可以理解为在某一点上函数值的变化情况（包括变化快慢及变化方向）。

对于任意一个函数![equation](http://latex.codecogs.com/gif.latex?f(x))，其在![equation](http://latex.codecogs.com/gif.latex?x_0)处的导数可以定义为：

![equation](http://latex.codecogs.com/gif.latex?f^{'}(x)=\frac{df(x)}{dx}=\lim_{h->0}\frac{f(x_0+h)-f(x_0)}{h})

下面不加证明的列出一些常用函数的导数形式及相关公式：

![equation](http://latex.codecogs.com/gif.latex?(ax^n)^{'}=\frac{d(ax^n)}{dx}=anx^{n-1})

![equation](http://latex.codecogs.com/gif.latex?(ln(x))^{'}=\frac{1}{x})

![equation](http://latex.codecogs.com/gif.latex?(e^x)^{'}=e^x)

![equation](http://latex.codecogs.com/gif.latex?(f(x)+g(x))^{'}=f^{'}(x)+g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)-g(x))^{'}=f^{'}(x)-g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(f(x)g(x))^{'}=f^{'}(x)g(x)+f(x)g^{'}(x))

![equation](http://latex.codecogs.com/gif.latex?(\frac{f(x)}{g(x)})^{'}=\frac{f^{'}(x)g(x)-f(x)g^{'}(x)}{(g(x))^2})

然后还有比较重要的链式法则，若有一个函数：

![equation](http://latex.codecogs.com/gif.latex?y=f(u))

![equation](http://latex.codecogs.com/gif.latex?u=g(x))

那么可以得到：

![equation](http://latex.codecogs.com/gif.latex?\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx})



在简单介绍这部分知识后，我们来到反向传播算法。

## 反向传播算法

对于任意一个运算，我们可以通过构建计算图，然后利用链式法则前向求导，从而求出每一个变量的梯度。举个例子,假如有函数：

![function](http://latex.codecogs.com/gif.latex?f(x,y,z)=(x+y)z)

且

![equation](http://latex.codecogs.com/gif.latex?x=-2,y=5,z=-4)

根据运算法则，我们首先要计算![equation](http://latex.codecogs.com/gif.latex?x+y)，再计算乘法，所以可以令：

![equation](http://latex.codecogs.com/gif.latex?q=x+y)

![equation](http://latex.codecogs.com/gif.latex?f(q,z)=qz)

根据上述运算顺序画出如下计算图

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/002.png)

首先前向计算得到：

![equation](http://latex.codecogs.com/gif.latex?q=x+y=-2+5=3)

![equation](http://latex.codecogs.com/gif.latex?f=qz=3\times{-4}=-12)

下面我们看看反向梯度是如何计算的，我们需要计算的是：

![equation](http://latex.codecogs.com/gif.latex?\nabla{f}=[\frac{df}{dx},\frac{df}{dy},\frac{df}{dz}])

根据函数式及求导公式，我们首先可以得出以下结论：

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dz}=(qz)^{'}=q=3)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dq}=(qz)^{'}=z=-4)

![equation](http://latex.codecogs.com/gif.latex?\frac{dq}{dx}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{dq}{dy}=1)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dx}=\frac{df}{dq}\frac{dq}{dx}=-4\times1=-4)

![equation](http://latex.codecogs.com/gif.latex?\frac{df}{dy}=\frac{df}{dq}\frac{dq}{dy}=-4\times1=4)

所以最终求得梯度为：

![equation](http://latex.codecogs.com/gif.latex?\nabla{f}=[\frac{df}{dx},\frac{df}{dy},\frac{df}{dz}]=[-4,-4,3])

有了梯度之后就可以使用梯度下降法对参数进行更新了。

## 稍微复杂的一个例子

上面介绍了及其简单的一个例子，下面是稍微复杂的例子，也是在实际中会经常用到的例子：逻辑回归的梯度求解。需要求解梯度的函数如下：

![equation](http://latex.codecogs.com/gif.latex?f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}})

然后我们将上述函数画成计算图（原课程中的图截图过来后不是很清晰，故重画了），通过换元将上面复杂的式子写成简单的式子（特别方便求导的式子）：

![equation](http://latex.codecogs.com/gif.latex?q_0=w_0x_0)

![equation](http://latex.codecogs.com/gif.latex?q_1=w_1x_1)

![equation](http://latex.codecogs.com/gif.latex?s_1=q_0+q_1)

![equation](http://latex.codecogs.com/gif.latex?s_2=s_1+w_2)

![equation](http://latex.codecogs.com/gif.latex?a_1=-s_2)

![equation](http://latex.codecogs.com/gif.latex?a_2=e^{a_1})

![equation](http://latex.codecogs.com/gif.latex?a_3=1+a_2)

![equation](http://latex.codecogs.com/gif.latex?a_4=\frac{1}{a_3})

根据上面过程画出来图形如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/003.png)

给定一组已知的输入，比如：

![equation](http://latex.codecogs.com/gif.latex?w_0=2,x_0=-1,w_1=-3,x_1=-2,w_2=-3)

我们可以前向计算上面计算图中的每一个位置的输出值如下：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture4/004.png)

下面根据链式法则来求![equation](http://latex.codecogs.com/gif.latex?w_0,x_0,w_1,x_1,w_2)的梯度，实际应用中只会对系数求梯度，输入变量是固定的。

![equation](http://latex.codecogs.com/gif.latex?\frac{da_4}{da_3}=-\frac{1}{a_3^2},\frac{da_3}{da_2}=1,\frac{da_2}{da_1}=e^{a_1},\frac{da_1}{ds_2}=-1)

通过上面的式子我们可以得到：

![equation](http://latex.codecogs.com/gif.latex?\frac{da_4}{ds_2}=\frac{da_4}{da_3}\frac{da_3}{da_2}\frac{da_2}{da_1}\frac{da_1}{ds_2}=\frac{e^{a_1}}{a_3^2}=)

