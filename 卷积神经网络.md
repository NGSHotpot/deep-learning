# 卷积神经网络

## 系列简介

本系列将介绍Stanford深度学习课程的主要内容，做以下说明：本系列不是对原视频翻译，而是对每一课的主要内容进行说明及补充，其中关于课程中的行政事务我们将不做说明。 由于水平有限，在文章中难免出现错误，希望各位多多包含，帮助指正。

本文为该课程第五篇，介绍卷积神经网络的一些细节。

## 主要内容

卷积神经网络在近年来大发神威，在多类学习任务中起了至关重要的作用，特别是计算机视觉方面的成就，让卷积神经网络得到长足发展。从2012年Hinton带领Alex等利用卷积神经网络在ImageNet竞赛中以绝对优势取得冠军之后，ImageNet图像分类任务一次又一次被卷积神经网络刷新着记录。2013年的Overfeat，2014年的VGG，GoogleNet，2015年的ResNet，以及Inception-v2，Inception-v3，Inception-ResNet等卷积网络结构的出现，不断的刷新的记录。这一篇主要介绍卷积神经网络的一些基本部件，帮助我们搞清楚卷积神经网络是如何工作的。

我们这部分说的可能和课程中的差异稍大一些，这部分主要介绍卷积网络中的常用部件，比如卷积层、pooling层、卷积的strides、卷积的padding以及卷积核参数数量的估计等内容。

## 卷积层

卷积层是卷积神经网络的核心，但本质上，卷积层并不复杂，通过下面一个例子我们大家就都会了：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/001.png)

想要做卷积操作，当然需要一个输入图片，当然也需要一个卷积核。如上图所示，左边大的方阵是输入图片，其大小为7x7，而右边小的3x3大小的方阵就是卷积核。卷积操作可以理解为卷积核的方阵在输入图片中滑动，每滑动到一个位置就进行一次卷进计算。下面是如何做卷积计算，当3x3大小的卷积核滑动到图片左上角红色数字区域时，卷积实际上就是将该卷积核与上面的红色数字区域重叠，然后将对应位置相乘，再将所有的值加起来就是一个卷积计算。下面是刚才说的这个例子的计算过程：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/002.png)

为了更清楚、更准确的理解，大家可以思考一下右下角的卷积应该如何计算，计算出来结果应该是多少？使用相同的方法，我们可以计算得到其卷积计算后的值，如下图所示

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/003.png)

卷积核变量完整个输入图片区域，并进行卷积计算就完成了一次卷积的操作，下图综合刚才说到的过程：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/004.png)

到这里，应该都懂了卷积是如何算的了。下面我们简单的拓展一下，方便理解我们实际的例子。刚才的例子中，我们的输入图片是一维的，但是显然我们的图片一般情况下都有RGB三维，对于多维的图片输入是如何计算的呢？

此时我们的卷积计算是一样的，我们仍然将卷积核重叠到输入图像中，并且对应数值相乘，然后再加和。但是由于输入图片有三个通道（R，G，B），所以卷积核也必须要有三个通道才能和输入图像进行一一对应。如下图所示，这样卷积计算和上面的例子是

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/005.png)

不仅是三维，对于多维的情况是一样的处理。那么上面的卷积后的输出是什么？根据刚才的说法，显然上面一通道的卷积和刚才说的三通道的卷积的输入大小是一样的，此时的输出只有单个通道。我们可以也使得卷积的输出也有多个通道吗？当然可以，我们只需要多用几个卷积核就可以了，如下图所示：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/006.png)

通常情况下卷积操作可以表示为：

![equation](http://latex.codecogs.com/gif.latex?I(batch_size,width,height,channel)*filter(w,h,c,n))

其中I是输入图像，batch_size代表做卷积的共有的图像数量，width代表图片的宽度，height代表图片的高度，channel代表图片的通道数（R,G,B的话就是3），另外filter是卷积核，卷积核中w代表卷积核宽度，h代表卷积核高度，c代表卷积核通道数，这里的卷积核必须等于输入图像的通道数，n代表卷积核的个数，也代表着输出图像的通道数。

上面介绍的都是卷积的计算，在卷积神经网络中的卷积层中，在卷积计算之后，还需要加上一个偏置bias，单个数值。

到这里，对于卷积的计算应该很清楚了，下面我们介绍卷积神经网络中的另外一个常用模块，pooling层。

## Pooling 层

Pooling层包括max-pooling和average-pooling等常用的pooling层，max-pooling可能稍微常用一下，所以介绍max pooling。对于一个pooling来说是需要事先指定好pooling的大小，如下图所示，pooling区域大小为2x2，然后max-pooling就是获取每一个2x2区域的最大值，并且输出。若是average-pooling，类似的，获取指定区域内的平均值，然后输出。很简单吧！

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/007.png)

## strides

在做卷积的时候，我们还有一个重要的参数叫做strides，我们再来看我们的第一个例子，刚才提到，做卷积的时候，我们是将卷积核在输入图像上滑动，所以如下图所示，我们在红框区域内做完卷积后，滑动到蓝框区域继续做卷积。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/008.png)

若是直接做完红框做蓝框，那么我们的strides的值就是1，但是若是像下图那样，做完红框之后做橙色框，那么此时strides就为2了。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/009.png)

所以strides控制的是卷积核在输入图像上滑动时每次移动的像素数。刚才我们只说到了横向移动，当然我们也可以纵向移动。刚才我们在说卷积的时候，说了卷积的输入是

![equation](http://latex.codecogs.com/gif.latex?I(batch_size,width,height,channel))

共有四个维度，所以strides一般也定义为思维，比如![equation](http://latex.codecogs.com/gif.latex?strides=[1,2,2,1]),代表卷积核在每一维上的移动情况。通常情况下，我们会设定strides的第一维和第四维都为1，第一维是batch_size，是输入的图像的数量，若是这一维的strides不为1，那么会跳过一些样本。第四维是输入图像的channel数，由于设定了卷积核的通道和输入图像的channel是相同，所以此时这一维的strides也为1。中间两维是属于超参数，可以用户自定义，一般情况下，若是卷积的网络的strides，设定为3x3,5x5,7x7,1x1等。若是pooling层的strides，设定为2x2,3x3,5x5，大家可以在实际项目中去测试。

## Padding

下面还有一个比较关键的变量是padding，还是最开始的例子：

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/004.png)

我们的输入图像是6x6大小，经过卷积运算后输出图像大小只有4x4。很容易想到，按照目前我们的卷积方式，卷积之后输入图像大小一定会变小（卷积核大小为1x1除外），那么有没有办法使得输出不变小呢？当然有，padding！padding是什么？padding就是在图像周围加数值，比如现在图像大小为6x6，我们如果在这个图像基础上加上一圈0，图像大小就会变为8x8，这个就是padding。

通常情况下，在卷积中有两种padding的方式，一种是valid，一种是same。从一开始到现在为止，我们讲的都是valid的padding，valid padding就是不padding。第二种padding就是same，same的含义是保持加在图像四周加padding，保证在stride为1的时候输入图像大小与输出图像大小一致。对于不同大小的卷积核，padding的情况是不一样的。比如，若是卷积核大小为3x3，same padding会在周围加上一圈的0，如下图所示。

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/010.png)

当卷积核大小为5x5时，此时的same padding在周围加上两圈0，如下图所示，

![](https://github.com/NGSHotpot/deep-learning/blob/master/stanford_img/lecture5/011.png)

不过我们完全不用担心这个，在所有的深度学习框架中，padding机制都做好，不需要我们指定padding多大范围，我们只需要确定是same还是valid的padding。

## 卷积网络参数数量估计

卷积层有参数，pooling层没有参数。

估计参数数量还是也是很重要的，拿课程中的一个例子来说，假如说输入图像的大小为32x32x3，我们使用10个5x5大小的卷积核去做卷积，总共有多少个参数呢？

根据前面说的卷积计算的定义，我们知道，卷积核的实际大小为5x5x3x10,其中5x5代表卷积核大小，3是通道数，必须和输入图像一致，10为卷积核的个数。所以总共有5x5x3x10=750个参数。

对吗？不对，虽然没有进行强调，但是我们在卷积层那一部分最后说到，在卷积后面还需要加上一个偏置，这个偏置也是参数，所以对于每一个卷积核还要额外加1,10个卷积核所以加10，总共750+10=760个参数。


## 索引

上一讲：[Stanford深度学习课程|第四课神经网络及反向传播算法](https://github.com/NGSHotpot/deep-learning/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.md)

下一讲：[Stanford深度学习课程|第六课神经网络的训练](https://github.com/NGSHotpot/deep-learning/blob/master/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83.md)
